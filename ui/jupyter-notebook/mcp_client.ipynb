{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP Server Client - Jupyter Notebook\n",
    "\n",
    "Interactive notebook for testing deployed MCP servers on GCP Cloud Run.\n",
    "\n",
    "**Features:**\n",
    "- Call 9 deployed MCP servers via Claude API\n",
    "- Run bioinformatics workflows interactively\n",
    "- Visualize results (spatial plots, pathway networks, heatmaps)\n",
    "- Track token usage and costs\n",
    "- Reproducible analysis workflows\n",
    "\n",
    "**Prerequisites:**\n",
    "- Python 3.11+\n",
    "- ANTHROPIC_API_KEY environment variable set\n",
    "- Internet connection (calls GCP Cloud Run servers)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Import libraries and configure MCP server connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install anthropic python-dotenv pandas matplotlib seaborn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key\n",
    "api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY not found. Set it in .env file or environment.\")\n",
    "\n",
    "print(\"âœ… API Key configured\")\n",
    "print(f\"âœ… Using Anthropic SDK version: {anthropic.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. MCP Server Configuration\n\nAll 9 MCP servers deployed on GCP Cloud Run."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# MCP Server URLs (all deployed on GCP Cloud Run)\nMCP_SERVERS = {\n    \"fgbio\": {\n        \"url\": \"https://mcp-fgbio-ondu7mwjpa-uc.a.run.app/sse\",\n        \"description\": \"Genomic reference data and FASTQ validation\",\n        \"status\": \"production\"\n    },\n    \"multiomics\": {\n        \"url\": \"https://mcp-multiomics-ondu7mwjpa-uc.a.run.app/sse\",\n        \"description\": \"Multi-omics integration (RNA/Protein/Phospho)\",\n        \"status\": \"production\"\n    },\n    \"spatialtools\": {\n        \"url\": \"https://mcp-spatialtools-ondu7mwjpa-uc.a.run.app/sse\",\n        \"description\": \"Spatial transcriptomics analysis\",\n        \"status\": \"production\"\n    },\n    \"tcga\": {\n        \"url\": \"https://mcp-tcga-ondu7mwjpa-uc.a.run.app/sse\",\n        \"description\": \"TCGA cancer genomics data\",\n        \"status\": \"mock\"\n    },\n    \"openimagedata\": {\n        \"url\": \"https://mcp-openimagedata-ondu7mwjpa-uc.a.run.app/sse\",\n        \"description\": \"H&E/MxIF image loading, registration, feature extraction, and composite generation\",\n        \"status\": \"production\"\n    },\n    \"deepcell\": {\n        \"url\": \"https://mcp-deepcell-ondu7mwjpa-uc.a.run.app/sse\",\n        \"description\": \"DeepCell-TF cell segmentation and marker quantification\",\n        \"status\": \"production\"\n    },\n    \"cell-classify\": {\n        \"url\": \"https://mcp-cell-classify-ondu7mwjpa-uc.a.run.app/sse\",\n        \"description\": \"Cell phenotype classification and visualization\",\n        \"status\": \"production\"\n    },\n    \"mockepic\": {\n        \"url\": \"https://mcp-mockepic-ondu7mwjpa-uc.a.run.app/sse\",\n        \"description\": \"Mock EHR/FHIR data\",\n        \"status\": \"mock\"\n    }\n}\n\n# Display available servers\nprint(\"Available MCP Servers:\\n\")\nfor name, config in MCP_SERVERS.items():\n    status_icon = \"+\" if config[\"status\"] == \"production\" else \"~\"\n    print(f\"{status_icon} {name}: {config['description']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "\n",
    "Utilities for calling MCP servers and processing responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCPClient:\n",
    "    \"\"\"Helper class for calling MCP servers via Claude API.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str = None):\n",
    "        self.api_key = api_key or os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        self.client = anthropic.Anthropic(api_key=self.api_key)\n",
    "        self.conversation_history = []\n",
    "        \n",
    "    def call_servers(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        servers: List[str],\n",
    "        model: str = \"claude-sonnet-4-5\",\n",
    "        max_tokens: int = 4096,\n",
    "        clear_history: bool = False\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Call MCP servers with a prompt.\n",
    "        \n",
    "        Args:\n",
    "            prompt: User query/instruction\n",
    "            servers: List of server names to enable\n",
    "            model: Claude model to use\n",
    "            max_tokens: Maximum response tokens\n",
    "            clear_history: Clear conversation history before call\n",
    "            \n",
    "        Returns:\n",
    "            Dict with response text, usage info, and metadata\n",
    "        \"\"\"\n",
    "        if clear_history:\n",
    "            self.conversation_history = []\n",
    "            \n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        })\n",
    "        \n",
    "        # Build MCP server configs\n",
    "        mcp_servers = [\n",
    "            {\n",
    "                \"type\": \"url\",\n",
    "                \"url\": MCP_SERVERS[server][\"url\"],\n",
    "                \"name\": server\n",
    "            }\n",
    "            for server in servers\n",
    "            if server in MCP_SERVERS\n",
    "        ]\n",
    "        \n",
    "        # Build tools config\n",
    "        tools = [\n",
    "            {\"type\": \"mcp_toolset\", \"mcp_server_name\": server}\n",
    "            for server in servers\n",
    "            if server in MCP_SERVERS\n",
    "        ]\n",
    "        \n",
    "        # Call Claude API\n",
    "        response = self.client.beta.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=max_tokens,\n",
    "            messages=self.conversation_history,\n",
    "            mcp_servers=mcp_servers,\n",
    "            tools=tools,\n",
    "            betas=[\"mcp-client-2025-11-20\"]\n",
    "        )\n",
    "        \n",
    "        # Extract response text\n",
    "        response_text = \"\"\n",
    "        for block in response.content:\n",
    "            if hasattr(block, 'text'):\n",
    "                response_text += block.text\n",
    "        \n",
    "        # Add assistant response to history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": response_text\n",
    "        })\n",
    "        \n",
    "        # Extract usage info\n",
    "        usage = {\n",
    "            \"input_tokens\": response.usage.input_tokens,\n",
    "            \"output_tokens\": response.usage.output_tokens,\n",
    "            \"total_tokens\": response.usage.input_tokens + response.usage.output_tokens,\n",
    "            \"estimated_cost_usd\": self._estimate_cost(\n",
    "                response.usage.input_tokens,\n",
    "                response.usage.output_tokens,\n",
    "                model\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"response\": response_text,\n",
    "            \"usage\": usage,\n",
    "            \"model\": model,\n",
    "            \"servers_used\": servers\n",
    "        }\n",
    "    \n",
    "    def _estimate_cost(self, input_tokens: int, output_tokens: int, model: str) -> float:\n",
    "        \"\"\"Estimate cost in USD based on token usage.\"\"\"\n",
    "        # Pricing as of Dec 2025 (approximate)\n",
    "        pricing = {\n",
    "            \"claude-sonnet-4-5\": {\"input\": 0.003 / 1000, \"output\": 0.015 / 1000},\n",
    "            \"claude-opus-4-5\": {\"input\": 0.015 / 1000, \"output\": 0.075 / 1000},\n",
    "            \"claude-haiku-4\": {\"input\": 0.001 / 1000, \"output\": 0.005 / 1000}\n",
    "        }\n",
    "        \n",
    "        rates = pricing.get(model, pricing[\"claude-sonnet-4-5\"])\n",
    "        cost = (input_tokens * rates[\"input\"]) + (output_tokens * rates[\"output\"])\n",
    "        return round(cost, 4)\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history.\"\"\"\n",
    "        self.conversation_history = []\n",
    "\n",
    "# Initialize client\n",
    "mcp = MCPClient()\n",
    "print(\"âœ… MCP Client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quick Test - List Available Tools\n",
    "\n",
    "Simple test to verify MCP server connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test: Ask about available tools\n",
    "result = mcp.call_servers(\n",
    "    prompt=\"What spatial transcriptomics analysis tools are available?\",\n",
    "    servers=[\"spatialtools\"],\n",
    "    clear_history=True\n",
    ")\n",
    "\n",
    "print(\"ðŸ“Š Response:\\n\")\n",
    "print(result[\"response\"])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(f\"ðŸ’° Cost: ${result['usage']['estimated_cost_usd']}\")\n",
    "print(f\"ðŸ“ˆ Tokens: {result['usage']['input_tokens']} in, {result['usage']['output_tokens']} out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Example Workflow: Spatial Transcriptomics Analysis\n",
    "\n",
    "Complete workflow for analyzing spatial transcriptomics data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow 1: Cell Type Deconvolution\n",
    "result = mcp.call_servers(\n",
    "    prompt=\"\"\"\n",
    "    Analyze the spatial transcriptomics data for Patient-001.\n",
    "    Perform cell type deconvolution and summarize the key cell populations.\n",
    "    \"\"\",\n",
    "    servers=[\"spatialtools\", \"mockepic\"],\n",
    "    clear_history=True\n",
    ")\n",
    "\n",
    "print(\"ðŸ“Š Cell Type Deconvolution Results:\\n\")\n",
    "print(result[\"response\"])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(f\"ðŸ’° Cost: ${result['usage']['estimated_cost_usd']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example Workflow: Pathway Enrichment\n",
    "\n",
    "Gene set enrichment analysis on differentially expressed genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow 2: Pathway Enrichment\n",
    "result = mcp.call_servers(\n",
    "    prompt=\"\"\"\n",
    "    For the following upregulated genes: TP53, BRCA1, MYC, KRAS, PTEN\n",
    "    Perform pathway enrichment analysis using GO_BP database.\n",
    "    Show the top 5 enriched pathways.\n",
    "    \"\"\",\n",
    "    servers=[\"spatialtools\"],\n",
    "    clear_history=True\n",
    ")\n",
    "\n",
    "print(\"ðŸ§¬ Pathway Enrichment Results:\\n\")\n",
    "print(result[\"response\"])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(f\"ðŸ’° Cost: ${result['usage']['estimated_cost_usd']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Example Workflow: Multi-Omics Integration\n",
    "\n",
    "Integrate RNA, protein, and phosphorylation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow 3: Multi-omics Integration\n",
    "result = mcp.call_servers(\n",
    "    prompt=\"\"\"\n",
    "    Explain the multi-omics analysis workflow available.\n",
    "    What tools can I use for integrating RNA, protein, and phosphorylation data?\n",
    "    \"\"\",\n",
    "    servers=[\"multiomics\"],\n",
    "    clear_history=True\n",
    ")\n",
    "\n",
    "print(\"ðŸ”¬ Multi-Omics Integration Capabilities:\\n\")\n",
    "print(result[\"response\"])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(f\"ðŸ’° Cost: ${result['usage']['estimated_cost_usd']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete PatientOne Workflow\n",
    "\n",
    "End-to-end precision medicine analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete workflow\n",
    "result = mcp.call_servers(\n",
    "    prompt=\"\"\"\n",
    "    For Patient-001 (ovarian cancer, platinum-resistant, on bevacizumab):\n",
    "    \n",
    "    1. Get clinical data\n",
    "    2. Retrieve spatial transcriptomics data\n",
    "    3. Perform cell type deconvolution\n",
    "    4. Identify key findings for treatment planning\n",
    "    \n",
    "    Provide a summary of actionable insights.\n",
    "    \"\"\",\n",
    "    servers=[\"mockepic\", \"spatialtools\"],\n",
    "    clear_history=True,\n",
    "    max_tokens=8192\n",
    ")\n",
    "\n",
    "print(\"ðŸ¥ PatientOne Complete Analysis:\\n\")\n",
    "print(result[\"response\"])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(f\"ðŸ’° Cost: ${result['usage']['estimated_cost_usd']}\")\n",
    "print(f\"ðŸ“ˆ Tokens: {result['usage']['total_tokens']} total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Token Usage Tracking\n",
    "\n",
    "Monitor API costs across multiple queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Track costs across multiple queries\n",
    "queries = [\n",
    "    (\"What tools are available for spatial analysis?\", [\"spatialtools\"]),\n",
    "    (\"Explain multi-omics integration methods.\", [\"multiomics\"]),\n",
    "    (\"What genomic reference data can I access?\", [\"fgbio\"])\n",
    "]\n",
    "\n",
    "total_cost = 0\n",
    "results = []\n",
    "\n",
    "for prompt, servers in queries:\n",
    "    result = mcp.call_servers(prompt, servers, clear_history=True)\n",
    "    total_cost += result[\"usage\"][\"estimated_cost_usd\"]\n",
    "    results.append({\n",
    "        \"query\": prompt[:50] + \"...\",\n",
    "        \"servers\": \", \".join(servers),\n",
    "        \"tokens\": result[\"usage\"][\"total_tokens\"],\n",
    "        \"cost_usd\": result[\"usage\"][\"estimated_cost_usd\"]\n",
    "    })\n",
    "\n",
    "# Display as DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "print(\"ðŸ’° Token Usage Summary:\\n\")\n",
    "print(df.to_string(index=False))\n",
    "print(f\"\\nðŸ“Š Total Cost: ${total_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualization Example (Optional)\n",
    "\n",
    "Visualize token usage across different queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize token usage\n",
    "if len(results) > 0:\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Token usage by query\n",
    "    ax1.barh(df['query'], df['tokens'], color='skyblue')\n",
    "    ax1.set_xlabel('Total Tokens')\n",
    "    ax1.set_title('Token Usage by Query')\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Cost by query\n",
    "    ax2.barh(df['query'], df['cost_usd'], color='lightcoral')\n",
    "    ax2.set_xlabel('Cost (USD)')\n",
    "    ax2.set_title('API Cost by Query')\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Average cost per query: ${df['cost_usd'].mean():.4f}\")\n",
    "    print(f\"ðŸ“Š Average tokens per query: {df['tokens'].mean():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Advanced: Custom Workflow\n",
    "\n",
    "Create your own analysis workflow by combining multiple MCP servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom workflow - modify this cell for your analysis\n",
    "your_prompt = \"\"\"\n",
    "Your custom analysis prompt here...\n",
    "\"\"\"\n",
    "\n",
    "your_servers = [\"spatialtools\", \"multiomics\"]  # Select relevant servers\n",
    "\n",
    "# Run your analysis\n",
    "result = mcp.call_servers(\n",
    "    prompt=your_prompt,\n",
    "    servers=your_servers,\n",
    "    clear_history=True\n",
    ")\n",
    "\n",
    "print(result[\"response\"])\n",
    "print(f\"\\nðŸ’° Cost: ${result['usage']['estimated_cost_usd']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides:\n",
    "- âœ… Direct access to 9 deployed MCP servers\n",
    "- âœ… Helper functions for calling servers and tracking costs\n",
    "- âœ… Example workflows for common bioinformatics tasks\n",
    "- âœ… Visualization of token usage and costs\n",
    "- âœ… Reproducible analysis environment\n",
    "\n",
    "**Next Steps:**\n",
    "- Modify workflows for your specific research questions\n",
    "- Combine multiple MCP servers for complex analyses\n",
    "- Export results to CSV/JSON for downstream processing\n",
    "- Deploy to GCP Vertex AI Workbench for team collaboration\n",
    "\n",
    "**Documentation:**\n",
    "- [Main README](../../README.md)\n",
    "- [MCP Server Docs](../../servers/)\n",
    "- [Anthropic API Docs](https://docs.anthropic.com/)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}